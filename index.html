<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<!-- saved from url=(0032)https://www.cs.cmu.edu/~dpathak/ -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=windows-1252">
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
  
  a {
  color: #1772d0;
  text-decoration:none;
  }
  a:focus, a:hover {
  color: #f09228;
  text-decoration:none;
  }
  body,td,th {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 16px;
    font-weight: 400
  }
  heading {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 17px; /* 19 */
    font-weight: 600 /* 1000 */
  }
  hr
  {
    border: 0;
    height: 1px;
    background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
  }
  strong {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 16px;
    font-weight: 600 /* 800 */
  }
  strongred {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    color: 'red' ;
    font-size: 16px
  }
  sectionheading {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 22px;
    font-weight: 600
  }
  pageheading {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 38px;
    font-weight: 400
  }
  .ImageBorder
  {
      border-width: 1px;
      border-color: Black;
  }
  span.highlight {
  background-color: #ffffd0;
  }

  </style>
  <link rel="shortcut icon" href="./documents/images/robot-white.png" type="image/vnd.microsoft.icon">
  <script async="" src="./Deepak Pathak_files/analytics.js"></script><script type="text/javascript" src="./js/hidebib.js"></script>
  <title>Cody Houff</title>
  <meta name="Cody Houff&#39;s Homepage" http-equiv="Content-Type" content="Cody Houff&#39;s Homepage">
  <link href="./css/font.css" rel="stylesheet" type="text/css">
  <!-- Start : Google Analytics Code -->
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', 'UA-64069893-1', 'auto');
    ga('send', 'pageview');
  </script>
  <!-- End : Google Analytics Code -->
  <!-- Scramble Script by Jeff Donahue -->
  <!-- <script src="./js00000/scramble.js"></script>-->
<!-- <script charset="utf-8" src="./js000000/button.e7f9415a2e000feaab02c86dd5802747.js"></script></head> -->

<body>
<table width="1000" border="0" align="center" cellspacing="0" cellpadding="20">
  <tbody><tr><td style="padding-bottom: 300px;">

<p align="center">
    <!-- <font size="7">Cody Houff</font><br> -->
    <pageheading>Cody Houff</pageheading><br>
    <b>email</b>:&nbsp;
    <font id="email" style="display:inline;">codysoccerman27@gmail.com</font><br>
    <b>academic email</b>:&nbsp;
    <font id="email" style="display:inline;">chouff3@gatech.edu</font>
  </p><table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  

  <tbody><tr>
    <td width="32%" valign="top"><a href="./documents/images/GTfall2021.jpg"><img src="./documents/images/GTfall2021.jpg" width="100%" style="border-radius:15px"></a>
    <p align="center">
    <a href="./documents/pdfs/Cody_Houff_Resume_V20.pdf" target="_blank">Resume</a> |
    <a href="https://scholar.google.com/citations?user=5RaRwlgAAAAJ&hl=en">Google Scholar</a> <br>
    <a href="https://github.com/codyhouff">Github</a> |
    <a href="https://www.linkedin.com/in/cody-houff-474a9017b/">LinkedIn</a> |
    <a href="https://twitter.com/codyhouff">Twitter</a> <br>
    </p><p align="center" style="margin-top:-8px;"><iframe id="twitter-widget-0" scrolling="no" frameborder="0" allowtransparency="true" allowfullscreen="true" class="twitter-follow-button twitter-follow-button-rendered" style="position: static; visibility: visible; width: 154px; height: 20px;" title="Twitter Follow Button" src="./documents/html/Twitter Follow Button.html" data-screen-name="codyhouff"></iframe><script async="" src="./js/widgets.js" charset="utf-8"></script></p>
    <p></p>
    </td>
    <td width="68%" valign="top" align="justify">
    <!-- good example: https://aliengirlliv.github.io/oliviawatkins/index.html-->
    <i>SOTA 100 trillion parameter model trained using ~10^25 FLOPs on a dataset of ~10^15 tokens</i>  
    
    <p>I am a graduate student pursuing a M.S. in robotics at Georgia Institute of Technology with a concentration in AI, computer vision, mechanics, and controls. I have approximately 3 years of work experience. I'm equipped with a good mix of ME, EE, and CS skills. My research experience is in computer vision, machine learning, engineering, &amp; robotics.</p>

    <p>I work as a research assistant in the Georgia Tech Robotics Lab under Charlie Kemp. In the lab, I apply machine learning to assistive household robotics. I've led projects, designed and trained models, implemented interpretability tools, collected and curated large datasets, and designed data capture hardware and protocol. Additionally, I led a lab reading group covering transformers, reinforcement learning, and robotics papers.</a> </p>

    <p><b>Opinion: </b>Let's make humans a multiplanetary species while we can. I predict we will have <a href="https://www.metaculus.com/questions/3479/date-weakly-general-ai-is-publicly-known/">Weak AGI</a> in a few years and <a href="https://www.metaculus.com/questions/5121/date-of-artificial-general-intelligence/">Strong AGI</a> in a decade or so. The field of <a href="https://www.metaculus.com/questions/3479/date-weakly-general-ai-is-publicly-known/">AI alignment</a> is extremely important.<br></p>

    </td>
  </tr>
</tbody></table>

<hr>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">

</tbody></table>



<!-- Publications -->
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tbody><tr><td><sectionheading>&nbsp;&nbsp;Publications </sectionheading></span></td></tr> <!-- (representative papers are <span class="highlight">highlighted -->
</tbody></table>
<br><table width="100%" align="center" border="0" cellspacing="0" cellpadding="15">

<!-- #1 -->
  <tbody><tr> <!--bgcolor="#ffffd0"-->
  <td width="40%" valign="top" align="center"><a href="https://codyhouff.github.io/AI_Progress/">
    <!-- <video autoplay="" loop="" muted="" src="./documents/video/vision-loco.mp4" alt="sym" style="padding-top:0px;padding-bottom:0px;border-radius:15px;" width="90%"></video> -->
    <img src="./documents/images/ai_progress_cropped.png" alt="ai_progress" style="padding-top:0px;padding-bottom:0px;border-radius:15px;" width="90%">

  </a></td>
  <td width="67%" valign="top">
    <p><a href="https://codyhouff.github.io/AI_Progress/" id="AI_Progress">
    <heading>Collection of Data on AI Progress</heading></a><br>
    Cody Houff<br>
    In Progress &nbsp;<b></b><br>
    <b></b>
    </p>

    <div class="paper" id="ai_progress">
    <a href="https://codyhouff.github.io/AI_Progress/">webpage</a> |
    <a href="javascript:toggleblock(&#39;ai_progress_abs&#39;)">summary</a> |
    <!-- <a shape="rect" href="javascript:togglebib(&#39;ai_progress&#39;)" class="togglebib">bibtex</a> | -->
    <!-- <a href="">arXiv</a> | -->
    <!-- <a href="">demo</a> | -->
    <a href="https://github.com/codyhouff/AI_Progress">code</a>

    <p align="justify" style="display: none;" id="ai_progress_abs">The beginning of this project arose from the scarcity of readily available data pertaining to the historical development of AI models, model costs, global AI progress comparisons, published AI papers, hardware progress, benchmark progress, forecasting data, and the potential implications of AI. It proved to be a frustrating task to access this data, which was either nonexistent or dispersed across disparate locations on the internet. An ideal solution would involve a centralized location that would present all the data in an accessible and comprehensible manner, through the medium of graphs and a downloadable excel sheet. In order to address this challenge, we have collated information from various sources, including Epoch's "Parameter Compute and Data Trends in Machine Learning," Open Philanthropy's "Forecasting TAI with Biological Anchors," and "How Much Computational Power Does It Take to Match the Human Brain?" in addition to data from other sources. Our objective is to develop a website that would serve as a reference point for AI alignment, forecasters, and individuals interested in AI to gain a comprehensive understanding of the current state of AI development and historical trends. This website would also facilitate the download and referencing of data by researchers. Additionally, it is our hope that this platform would encourage the general public to take AI alignment seriously, as they would be able to visualize and appreciate the progress made in AI over the years.</p>

<pre xml:space="preserve" style="display:none;">  N/A
</pre>
    </div>
  </td>
</tr>

<!-- #2 -->
<tbody><tr>
  <td width="40%" valign="top" align="center"><a href="https://github.com/codyhouff/imitation-pressure">
  <video autoplay="" loop="" muted="" src="./documents/video/transformer_robot_learning.mp4" alt="sym" style="padding-top:0px;padding-bottom:0px;border-radius:15px;" width="90%"></video>
  </a></td>
  <td width="67%" valign="top">
    <p><a href="https://github.com/codyhouff/imitation-pressure" id="TRANSFORMER_ROBOT_LEARNING">
    <heading>Text Conditioned Robot Task Planner and Executor</heading></a><br>
    Cody Houff, Jeremy A. Collins, Patrick Grady, Charles C. Kemp<br>
    In Progress, ~CoRL 2023 &nbsp;<b></b><br>
    <b></b>
    </p>

    <div class="paper" id="transformer_robot_learning">
    <a href="https://github.com/codyhouff/imitation-pressure">webpage</a> |
    <a href="javascript:toggleblock(&#39;transformer_robot_learning_abs&#39;)">summary</a> |
    <!-- <a shape="rect" href="javascript:togglebib(&#39;transformer_robot_learning&#39;)" class="togglebib">bibtex</a> | -->
    <!-- <a href="">arXiv</a> | -->
    <!-- <a href="">demo</a> | -->
    <a href="https://github.com/codyhouff/imitation-pressure">code</a>

    <p align="justify" style="display: none;" id="transformer_robot_learning_abs">A model that given text such as "open drawer" is able to plan visually it's next moves or actions and then execute them.</p>

<pre xml:space="preserve" style="display:none;">  N/A
</pre>
    </div>
  </td>
</tr>

<!-- #3 -->
<tbody><tr>
  <td width="40%" valign="top" align="center"><a href="https://www.youtube.com/watch?v=z5Rttv1oZJA">
  <video autoplay="" loop="" muted="" src="./documents/video/viper_overview.mp4" alt="sym" style="padding-top:0px;padding-bottom:0px;border-radius:15px;" width="90%"></video>
  </a></td>
  <td width="67%" valign="top">
    <p><a href="https://www.youtube.com/watch?v=z5Rttv1oZJA" id="VISUAL_PRESSURE">
    <heading>Visual Contact Pressure Estimation for Grippers in the Wild</heading></a><br>
    Jeremy A. Collins, Cody Houff, Patrick Grady, Charles C. Kemp<br>
    IROS 2023 &nbsp;<b></b><br>
    <b></b>
    </p>

    <div class="paper" id="visual_pressure">
    <a href="https://github.com/jeremy-collins/werp">webpage</a> |
    <a href="javascript:toggleblock(&#39;visual_pressure_abs&#39;)">abstract</a> |
    <a shape="rect" href="javascript:togglebib(&#39;visual_pressure&#39;)" class="togglebib">bibtex</a> |
    <a href="https://arxiv.org/abs/2303.07344">arXiv</a> |
    <a href="https://www.youtube.com/watch?v=z5Rttv1oZJA">demo</a> |
    <a href="https://github.com/jeremy-collins/werp">code</a>

    <p align="justify" style="display: none;" id="visual_pressure_abs">Sensing contact pressure applied by a gripper is useful for autonomous and teleoperated robotic manipulation, but adding tactile sensing to a gripper's surface can be difficult or impractical. If a gripper visibly deforms when forces are applied, contact pressure can be visually estimated using images from an external camera that observes the gripper. While researchers have demonstrated this capability in controlled laboratory settings, prior work has not addressed challenges associated with visual pressure estimation in the wild, where lighting, surfaces, and other factors vary widely. We present a deep learning model and associated methods that enable visual pressure estimation under widely varying conditions. Our model, Visual Pressure Estimation for Robots (ViPER), takes an image from an eye-in-hand camera as input and outputs an image representing the pressure applied by a soft gripper. Our key insight is that force/torque sensing can be used as a weak label to efficiently collect training data in settings where pressure measurements would be difficult to obtain. When trained on this weakly labeled data combined with fully labeled data containing pressure measurements, ViPER outperforms prior methods, enables precision manipulation in cluttered settings, and provides accurate estimates for unseen conditions relevant to in-home use.</p>

<pre xml:space="preserve" style="display:none;">  @misc{collins2023visual,
  title={Visual Contact Pressure Estimation 
  for Grippers in the Wild},
  author={Jeremy A. Collins and Cody Houff 
  and Patrick Grady and Charles C. Kemp},
  year={2023},
  eprint={2303.07344},
  archivePrefix={arXiv},
  primaryClass={cs.RO}
}
</pre>
    </div>
  </td>
</tr>

</tbody></table>
<hr>

<!-- Projects -->
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tbody><tr><td><sectionheading>&nbsp;&nbsp;Projects</sectionheading></td></tr> <!-- (representative papers are <span class="highlight">highlighted -->
</tbody></table>
<br><table width="100%" align="center" border="0" cellspacing="0" cellpadding="15">

<!-- #1 -->
<tbody><tr>
  <td width="40%" valign="top" align="center"><a href="./documents/images/electric_bike.jpg">
  <!-- <video autoplay="" loop="" muted="" src="./documents/video/vision-loco.mp4" alt="sym" style="padding-top:0px;padding-bottom:0px;border-radius:15px;" width="90%"></video> -->
  <img src="./documents/images/electric_bike.jpg" alt="life_in_weeks" style="padding-top:0px;padding-bottom:0px;" width="90%">

  </a></td>
  <td width="67%" valign="top">
    <p><a href="./documents/images/electric_bike.jpg" id="ELECTRIC_BIKE">
    <heading>Custom Built 60 MPH Electric Bike V1</heading></a><br>
    Cody Houff<br>
    In Progress &nbsp;<b></b><br>
    <b></b>
    </p>

    <div class="paper" id="electric_bike">
    <!-- <a href="https://codyhouff.github.io/electric_bike/">webpage</a> | -->
    <a href="javascript:toggleblock(&#39;electric_bike_abs&#39;)">summary</a>
    <!-- <a shape="rect" href="javascript:togglebib(&#39;life_in_weeks&#39;)" class="togglebib">bibtex</a> | -->
    <!-- <a href="https://arxiv.org/abs/2211.07638">arXiv</a> | -->
    <!-- <a href="https://youtu.be/5sRqythe6TE">demo</a> | -->
    <!-- <a href="https://github.com/codyhouff/electric_bike">code</a> -->

    <p align="justify" style="display: none;" id="electric_bike_abs">60mph bike with a 72V 3000W motor with a 72V 26.1AH battery</p>

<pre xml:space="preserve" style="display:none;">  N/A
</pre>
    </div>
  </td>
</tr>

<!-- #1 -->
<tbody><tr>
  <td width="40%" valign="top" align="center"><a href="https://codyhouff.github.io/Life_in_weeks/">
  <!-- <video autoplay="" loop="" muted="" src="./documents/video/vision-loco.mp4" alt="sym" style="padding-top:0px;padding-bottom:0px;border-radius:15px;" width="90%"></video> -->
  <img src="./documents/images/life_in_weeks.png" alt="life_in_weeks" style="padding-top:0px;padding-bottom:0px;border-radius:15px;" width="90%">

  </a></td>
  <td width="67%" valign="top">
    <p><a href="https://codyhouff.github.io/Life_in_weeks/" id="LIFE_IN_WEEKS">
    <heading>My Life in Weeks - Interactive</heading></a><br>
    Cody Houff<br>
    In Progress &nbsp;<b></b><br>
    <b></b>
    </p>

    <div class="paper" id="ai_progress">
    <a href="https://codyhouff.github.io/Life_in_weeks/">webpage</a> |
    <a href="javascript:toggleblock(&#39;life_in_weeks_abs&#39;)">summary</a> |
    <!-- <a shape="rect" href="javascript:togglebib(&#39;life_in_weeks&#39;)" class="togglebib">bibtex</a> | -->
    <!-- <a href="https://arxiv.org/abs/2211.07638">arXiv</a> | -->
    <!-- <a href="https://youtu.be/5sRqythe6TE">demo</a> | -->
    <a href="https://github.com/codyhouff/Life_in_weeks">code</a>

    <p align="justify" style="display: none;" id="life_in_weeks_abs">A visual life in weeks chart is an effective tool for planning and tracking personal goals and milestones. By breaking down an individual's life into weeks, it allows for a clear visualization of time and encourages goal setting and prioritization. Adding personal goals, bucket list items, and career goals to the chart not only helps with focus and motivation but also provides a roadmap for achieving long-term aspirations. Additionally, incorporating predicted technology advances into the chart enables individuals to plan for the future and stay ahead of emerging trends. By having a clear understanding of upcoming advancements and their potential impact on various aspects of life, individuals can make informed decisions and adapt their goals and plans accordingly. In summary, a visual life in weeks chart is a powerful tool for personal development and future planning, and adding markers for goals, bucket list items, career goals, and technology advances can enhance its effectiveness and relevance.</p>

<pre xml:space="preserve" style="display:none;">  N/A
</pre>
    </div>
  </td>
</tr>

<!-- #2 -->
<tbody><tr>
  <td width="40%" valign="top" align="center"><a href="https://github.com/jeremy-collins/vroom">
  <video autoplay="" loop="" muted="" src="./documents/video/video_demonstrations_small.mp4" alt="sym" style="padding-top:0px;padding-bottom:0px;border-radius:15px;" width="90%"></video>
  </a></td>
  <td width="67%" valign="top">
    <p><a href="https://github.com/jeremy-collins/vroom" id="VIDEO_DEMONSTRATIONS">
    <heading>Learning Robot Tasks from Video Demonstrations</heading></a><br>
    Cody Houff, Jeremy Collins, Alan Hesu, Dane Wang<br>
    2022 &nbsp;<b></b><br>
    <b></b>
    </p>

    <div class="paper" id="video_demonstrations">
    <!-- <a href="https://github.com/jeremy-collins/vroom">webpage</a> | -->
    <a href="javascript:toggleblock(&#39;video_demonstrations_abs&#39;)">abstract</a> |
    <a shape="rect" href="javascript:togglebib(&#39;video_demonstrations&#39;)" class="togglebib">bibtex</a> |
    <a href="./documents/pdfs/video_demonstrations.pdf">pdf</a> |
    <a href="https://www.youtube.com/watch?v=KbrXWbp9Awc">demo</a> |
    <a href="https://github.com/jeremy-collins/vroom">code</a>

    <p align="justify" style="display: none;" id="video_demonstrations_abs">Training an agent to solve a wide variety of complextasks has remained an open problem for decades; long-horizon, reward-sparse environments are notoriously diffi-cult to learn from scratch via reinforcement learning. Onesuch domain is robotic manipulation, which has been along-standing problem facing classical control theoristsand roboticists alike. At the core of this problem is the curseof dimensionality – the space of possible trajectories is solarge that a complete search through this space to learn apolicy is intractable for even short-horizon tasks. Addition-ally, very precise sensing is often required for robustness butoften leads to expensive solutions that are prone to hard-ware failure. We present an alternative approach that usesvisual data alone to learn a control policy for a robotic armby observing expert video demonstrations. We implementand test several models, accomplishing an 85% success fora pick-and-place task.</p>

<pre xml:space="preserve" style="display:none;">  N/A
</pre>
    </div>
  </td>
</tr>

<!-- #3 -->
<tbody><tr>
  <td width="40%" valign="top" align="center"><a href="https://vision-locomotion.github.io/">
  <video autoplay="" loop="" muted="" src="./documents/video/combination_rl_wide.mp4" alt="sym" style="padding-top:0px;padding-bottom:0px;border-radius:15px;" width="90%"></video>
  </a></td>
  <td width="67%" valign="top">
    <p><a href="https://vision-locomotion.github.io/" id="COMBINATION_RL">
    <heading>Combination and Benchmark of RL Models</heading></a><br>
    Cody Houff, Dane Wang, Etienne Sudre<br>
    2022 &nbsp;<b></b><br>
    <b></b>
    </p>

    <div class="paper" id="combination_rl">
    <!-- <a href="">webpage</a> | -->
    <a href="javascript:toggleblock(&#39;combination_rl_abs&#39;)">summary</a> |
    <!-- <a shape="rect" href="javascript:togglebib(&#39;combination_rl&#39;)" class="togglebib">bibtex</a> | -->
    <a href="./documents/pdfs/combination_and_benchmark_RL_models.pdf">pdf</a> |
    <a href="./documents/powerpoint/combination_rl.pptx">powerpoint</a> 
    <!-- <a href="">demo</a> | -->
    <!-- <a href="">code</a> -->

    <p align="justify" style="display: none;" id="combination_rl_abs">Atari games have been a long-standing benchmark in thereinforcement learning (RL) community for the past decade. This benchmark was proposed to test general competency of RL algorithms. In this environment, the frames of the game are inputted into the algorithms as observations. The RL community turned toward Deep Learning and Neural Networks in 2013 leading to multiple breakthroughs in algorithm development. In this project, we wanted to understand what part some of those algorithms played in the development of the current state of the art algorithms. To do so, we benchmarked several advanced and classical algorithms on the same Atari game, Space Invaders, where a laser cannon tries to defend Earth against invading aliens. To better understand the properties and performances of different algorithms, we extracted base algorithms from Rainbow (an algorithm that combines six different improved DQN algorithms). We would also like to explore new combinations and create our own RL models, so we implemented three combination algorithms that have not been originally explored. We tested Rainbow, these baselines algorithms, and our customized algorithms on Cartpole which is a simple but classic environment for RL.</p>

<pre xml:space="preserve" style="display:none;">  N/A
</pre>
    </div>
  </td>
</tr>

<!-- #4 -->
<tbody><tr>
  <td width="40%" valign="top" align="center"><a href="./documents/pdfs/sentiment_analysis.pdf">
  <!-- <video autoplay="" loop="" muted="" src="./documents/video/vision-loco.mp4" alt="sym" style="padding-top:0px;padding-bottom:0px;border-radius:15px;" width="90%"></video> -->
  <img src="./documents/images/sentiment_analysis_cropped.JPG" alt="life_in_weeks" style="padding-top:0px;padding-bottom:0px;border-radius:15px;" width="90%">
  </a></td>
  <td width="67%" valign="top">
    <p><a href="./documents/pdfs/sentiment_analysis.pdf" id="SENTIMENT_ANALYSIS">
    <heading>NLP Sentiment Analysis</heading></a><br>
    Cody Houff<br>
    2022 &nbsp;<b></b><br>
    <b></b>
    </p>

    <div class="paper" id="sentiment_analysis">
    <!-- <a href="https://vision-locomotion.github.io/">webpage</a> | -->
    <a href="javascript:toggleblock(&#39;sentiment_analysis_abs&#39;)">summary</a> |
    <!-- <a shape="rect" href="javascript:togglebib(&#39;sentiment_analysis&#39;)" class="togglebib">bibtex</a> | -->
    <a href="./documents/pdfs/sentiment_analysis.pdf">pdf</a> |
    <a href="./documents/powerpoint/sentiment_analysis.pptx">powerpoint</a> 
    <!-- <a href="https://vision-locomotion.github.io/#press-coverage">code</a> -->

    <p align="justify" style="display: none;" id="sentiment_analysis_abs">In the last decade there has been a large growing collection of opinions on the Internet, and especially on social media. This can be quite useful for policymakers, individuals, or organizations to understand the needs and problems of societies and formulate effective strategies for addressing them. Today many individuals argue there exists a tangible disconnect of various companies, figure heads, and politicians with the general population. In particular, the younger generation seems to be particularly disconnected. This is evident when browsing social networking sites such as Facebook, Instagram, Twitter, Reddit and reading the discussions. Quantifying this sentiment is difficult using traditional means. How does one quantify approval or disapproval from social media or news sources? Traditional polling methods or study groups can be slow, expensive, and sometimes ostracize a portion of the population. For example, calling only voters or customers with landlines can result in a poor sample of likely because some demographic groups have few landlines. Monitoring social networks represents a potential positive addition to the above methods. This method for capturing people's opinions overcomes the low-response rate problem and other problems that can arise from polling. People that use csocial networks naturally express their preferences in online discussions without being exposed to direct questions. This data can be collected with machine learning through sentiment analysis. Sentiment analysis is the use of natural language processing to systematically identify, extract, quantify, and study subjective information. My hope is that through deep learning sentiment analysis policymakers, individuals, or organizations can more effectively take calculated actions and create policies in line with public sentiment.</p>

<pre xml:space="preserve" style="display:none;">  N/A
</pre>
    </div>
  </td>
</tr>

<!-- #5 -->
<tbody><tr>
  <td width="40%" valign="top" align="center"><a href="">
  <video autoplay="" loop="" muted="" src="./documents/video/maze_robot.mp4" alt="sym" style="padding-top:0px;padding-bottom:0px;border-radius:15px;" width="90%"></video>
  </a></td>
  <td width="67%" valign="top">
    <p><a href="" id="MAZE_ROBOT">
    <heading>ROS Robot Maze</heading></a><br>
    Cody Houff<br>
    2022 &nbsp;<b></b><br>
    <b></b>
    </p>

    <div class="paper" id="maze_robot">
    <!--  <a href="">webpage</a> | -->
    <a href="javascript:toggleblock(&#39;maze_robot_abs&#39;)">summary</a> 
    <!-- <a shape="rect" href="javascript:togglebib(&#39;maze_robot&#39;)" class="togglebib">bibtex</a> | -->
    <!-- <a href="https://arxiv.org/abs/2303.07344">arXiv</a> | -->
    <!-- <a href="">demo</a> | -->
    <!-- <a href="">code</a> -->

    <p align="justify" style="display: none;" id="maze_robot_abs">The robot was placed in a maze. Used KNN, OpenCV, and ROS to find the goal in the shortest path possible dictated. Used computer vision techniques to navigate with the signs in the maze. The location of the goal and the starting position was not known ahead of time. Signs throughout the maze directed the robot from the starting position to the goal position.</p>

<pre xml:space="preserve" style="display:none;">  N/A
</pre>
    </div>
  </td>
</tr>

<!-- #6 -->
<tbody><tr>
  <td width="40%" valign="top" align="center"><a href="https://github.com/codyhouff/line_follower_robot_PID_control">
  <video autoplay="" loop="" muted="" src="./documents/video/pid_line_sideways.mp4" alt="sym" style="padding-top:0px;padding-bottom:0px;border-radius:15px;" width="90%"></video>
  </a></td>
  <td width="67%" valign="top">
    <p><a href="https://github.com/codyhouff/line_follower_robot_PID_control" id="PID_LINE">
    <heading>Embedded Programming for Line Follower Robot using PID Control</heading></a><br>
    Cody Houff<br>
    2021 &nbsp;<b></b><br>
    <b></b>
    </p>

    <div class="paper" id="pid_line">
    <!--  <a href="">webpage</a> | -->
    <a href="javascript:toggleblock(&#39;pid_line_abs&#39;)">summary</a> | 
    <!-- <a shape="rect" href="javascript:togglebib(&#39;pid_line&#39;)" class="togglebib">bibtex</a> | -->
    <!-- <a href="https://arxiv.org/abs/2303.07344">arXiv</a> | -->
    <!-- <a href="">demo</a> | -->
    <a href="https://github.com/codyhouff/line_follower_robot_PID_control">code</a>

    <p align="justify" style="display: none;" id="pid_line_abs"></p>

<pre xml:space="preserve" style="display:none;">  N/A
</pre>
    </div>
  </td>
</tr>

<!-- #7 -->
<tbody><tr>
  <td width="40%" valign="top" align="center"><a href="https://github.com/codyhouff/Tensorflow_object_detection_and_artificial_dataset_generation">
  <video autoplay="" loop="" muted="" src="./documents/video/do_not_enter_sign.mp4" alt="sym" style="padding-top:0px;padding-bottom:0px;border-radius:15px;" width="90%"></video>
  </a></td>
  <td width="67%" valign="top">
    <p><a href="https://github.com/codyhouff/Tensorflow_object_detection_and_artificial_dataset_generation" id="ARTIFICIAL_DATASET_GENERATION">
    <heading>General Purpose Object Detection from a Single Image using Artificial Dataset Generation</heading></a><br>
    Cody Houff<br>
    2019 &nbsp;<b></b><br>
    <b></b>
    </p>

    <div class="paper" id="artificial_dataset_generation">
    <!-- <a href="https://github.com/codyhouff/Tensorflow_object_detection_and_artificial_dataset_generation">webpage</a> | -->
    <a href="javascript:toggleblock(&#39;artificial_dataset_generation_abs&#39;)">summary</a> |
    <!-- <a shape="rect" href="javascript:togglebib(&#39;artificial_dataset_generation&#39;)" class="togglebib">bibtex</a> | -->
    <!-- <a href="">arXiv</a> | -->
    <!-- <a href="">demo</a> | -->
    <a href="https://github.com/codyhouff/Tensorflow_object_detection_and_artificial_dataset_generation">code</a>

    <p align="justify" style="display: none;" id="artificial_dataset_generation_abs">The development of an artificial dataset generator that can produce and autolabel thousands of images in a matter of seconds offers a revolutionary solution to the challenges of finding and labeling large datasets manually. This project's unique feature is its general-purpose nature, requiring only one image per class of object, eliminating the need for labeling efforts required by traditional object detectors. By offering a practical and efficient solution to this long-standing problem, this project has the potential to significantly improve the accuracy and efficiency of computer vision algorithms, with applications in various fields, including image recognition, robotics, and autonomous driving.</p>

<pre xml:space="preserve" style="display:none;">  N/A
</pre>
    </div>
  </td>
</tr>

<!-- #8 -->
<tbody><tr>
  <td width="40%" valign="top" align="center"><a href="">
  <video autoplay="" loop="" muted="" src="./documents/video/gocart_cropped.mp4" alt="sym" style="padding-top:0px;padding-bottom:0px;border-radius:15px;" width="90%"></video>
  </a></td>
  <td width="67%" valign="top">
    <p><a href="" id="GO_CART">
    <heading>Modular Dual Electric and Gas Go-Cart</heading></a><br>
    Cody Houff<br>
    2019 &nbsp;<b></b><br>
    <b></b>
    </p>

    <div class="paper" id="go_cart">
    <!--  <a href="">webpage</a> | -->
    <a href="javascript:toggleblock(&#39;go_cart_abs&#39;)">summary</a> 
    <!-- <a shape="rect" href="javascript:togglebib(&#39;go_cart&#39;)" class="togglebib">bibtex</a> | -->
    <!-- <a href="https://arxiv.org/abs/2303.07344">arXiv</a> | -->
    <!-- <a href="">demo</a> | -->
    <!-- <a href="">code</a> -->

    <p align="justify" style="display: none;" id="go_cart_abs">My senior design project involving the Modular Dual Electric and Gas Go-Cart, designed to facilitate easy assembly and disassembly of parts, represents a combined application of mechanical, electrical, and computer science skills. The ability to quickly build on or take off parts allowed for easy customization and modification, will provide ample opportunities for future innovation and development.</p>

<pre xml:space="preserve" style="display:none;">  N/A
</pre>
    </div>
  </td>
</tr>

<!-- #9 -->
<tbody><tr>
  <td width="40%" valign="top" align="center"><a href="">
  <video autoplay="" loop="" muted="" src="./documents/video/lego_robot_small.mp4" alt="sym" style="padding-top:0px;padding-bottom:0px;border-radius:15px;" width="90%"></video>
  </a></td>
  <td width="67%" valign="top">
    <p><a href="" id="LEGO_ROBOT">
    <heading>Embedded Programming for Pick and Place Robotic Arm</heading></a><br>
    Cody Houff<br>
    2019 &nbsp;<b></b><br>
    <b></b>
    </p>

    <div class="paper" id="lego_robot">
    <!--  <a href="">webpage</a> | -->
    <a href="javascript:toggleblock(&#39;lego_robot_abs&#39;)">summary</a> |
    <!-- <a shape="rect" href="javascript:togglebib(&#39;lego_robot&#39;)" class="togglebib">bibtex</a> | -->
    <!-- <a href="https://arxiv.org/abs/2303.07344">arXiv</a> | -->
    <!-- <a href="">demo</a> | -->
    <a href="https://github.com/codyhouff/Arduino_Pick_And_Place_Robotic_Arm">code</a>

    <p align="justify" style="display: none;" id="lego_robot_abs">Reverse engineered Mindstorms NXT sensors, and servo motors to work with a microcontoller. Developed and implemented embedded programming for the pick-and-place robot utilizing the micocontroller. Built a circuit to allow the microcontroller to interact with the servo motors.</p>

<pre xml:space="preserve" style="display:none;">  N/A
</pre>
    </div>
  </td>
</tr>


</tbody></table>
<hr>

<!-- Work Experience -->
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tbody><tr><td><sectionheading>&nbsp;&nbsp;Work Experience</sectionheading></td></tr> <!-- (representative papers are <span class="highlight">highlighted -->
</tbody></table>
<br><table width="100%" align="center" border="0" cellspacing="0" cellpadding="15">


<!-- #1 -->
<tbody><tr>
  <td width="40%" valign="top" align="center"><a href="https://www.linkedin.com/in/cody-houff-474a9017b/">
  <!-- <video autoplay="" loop="" muted="" src="./documents/video/vision-loco.mp4" alt="sym" style="padding-top:0px;padding-bottom:0px;border-radius:15px;" width="90%"></video> -->
  <img src="./documents/images/gra.png" alt="gra" style="padding-top:0px;padding-bottom:0px;border-radius:15px;" width="90%">

  </a></td>
  <td width="67%" valign="top">
    <p><a href="https://www.linkedin.com/in/cody-houff-474a9017b/" id="GRA">
    <heading>Graduate Research Assistant - Robotics Lab</heading></a><br>
    2022 - Current&nbsp;<b></b><br>
    <b></b>
    </p>

    <div class="paper" id="gra">
    <!-- <a href="https://codyhouff.github.io/Life_in_weeks/">webpage</a> |-->
    <a href="javascript:toggleblock(&#39;gra_abs&#39;)">summary</a>
    <!-- <a shape="rect" href="javascript:togglebib(&#39;life_in_weeks&#39;)" class="togglebib">bibtex</a> | -->
    <!-- <a href="https://arxiv.org/abs/2211.07638">arXiv</a> | -->
    <!-- <a href="https://youtu.be/5sRqythe6TE">demo</a> | -->
    <!-- <a href="https://github.com/codyhouff/Life_in_weeks">code</a> -->

    <p align="justify" style="display: none;" id="gra_abs">Applied machine learning to robotics.<br>Led projects, designed and trained models, implemented interpretability tools, collected and curated video datasets, designed data capture hardware and protocol.<br>Led a lab reading group focusing on transformers, RL, and current robotics papers.</p>
    <!-- <p align="justify" style="display: none;" id="gra_abs"><ul><li>Applied machine learning to robotics.</li> <li>Led projects, designed and trained models, implemented interpretability tools, collected and curated video datasets, designed data capture hardware and protocol.</li> <li>Led a lab reading group focusing on transformers, RL, and current robotics papers.</li> </ul> </p> -->

<pre xml:space="preserve" style="display:none;">  N/A
</pre>
    </div>
  </td>
</tr>

<!-- #2 -->
<tbody><tr>
  <td width="40%" valign="top" align="center"><a href="https://www.linkedin.com/in/cody-houff-474a9017b/">
  <!-- <video autoplay="" loop="" muted="" src="./documents/video/vision-loco.mp4" alt="sym" style="padding-top:0px;padding-bottom:0px;border-radius:15px;" width="90%"></video> -->
  <img src="./documents/images/white.png" alt="ego_robotics" style="padding-top:0px;padding-bottom:0px;border-radius:15px;" width="90%">

  </a></td>
  <td width="67%" valign="top">
    <p><a href="https://www.linkedin.com/in/cody-houff-474a9017b/" id="EGO_PRODUCTS">
    <heading>Project Lead Robotics Engineer - E.G.O. Products</heading></a><br>
    Summer 2022&nbsp;<b></b><br>
    <b></b>
    </p>

    <div class="paper" id="ego_robotics">
    <!-- <a href="https://codyhouff.github.io/Life_in_weeks/">webpage</a> |-->
    <a href="javascript:toggleblock(&#39;ego_robotics_abs&#39;)">summary</a>
    <!-- <a shape="rect" href="javascript:togglebib(&#39;life_in_weeks&#39;)" class="togglebib">bibtex</a> | -->
    <!-- <a href="https://arxiv.org/abs/2211.07638">arXiv</a> | -->
    <!-- <a href="https://youtu.be/5sRqythe6TE">demo</a> | -->
    <!-- <a href="https://github.com/codyhouff/Life_in_weeks">code</a> -->

    <p align="justify" style="display: none;" id="ego_robotics_abs">Applied machine learning to robotics. <br>Led projects, designed and trained models, implemented interpretability tools, collected and curated video datasets, designed data capture hardware and protocol. <br>Led a lab reading group focusing on transformers, RL, and current robotics papers </p>

<pre xml:space="preserve" style="display:none;">  N/A
</pre>
    </div>
  </td>
</tr>

<!-- #3 -->
<tbody><tr>
  <td width="40%" valign="top" align="center"><a href="https://www.linkedin.com/in/cody-houff-474a9017b/">
  <!-- <video autoplay="" loop="" muted="" src="./documents/video/vision-loco.mp4" alt="sym" style="padding-top:0px;padding-bottom:0px;border-radius:15px;" width="90%"></video> -->
  <img src="./documents/images/white.png" alt="gra" style="padding-top:0px;padding-bottom:0px;border-radius:15px;" width="90%">

  </a></td>
  <td width="67%" valign="top">
    <p><a href="https://www.linkedin.com/in/cody-houff-474a9017b/" id="GRA">
    <heading>Project Lead Engineer - Johnson Controls</heading></a><br>
    2020 - 2021&nbsp;<b></b><br>
    <b></b>
    </p>

    <div class="paper" id="gra">
    <!-- <a href="https://codyhouff.github.io/Life_in_weeks/">webpage</a> |-->
    <a href="javascript:toggleblock(&#39;life_in_weeks_abs&#39;)">summary</a>
    <!-- <a shape="rect" href="javascript:togglebib(&#39;life_in_weeks&#39;)" class="togglebib">bibtex</a> | -->
    <!-- <a href="https://arxiv.org/abs/2211.07638">arXiv</a> | -->
    <!-- <a href="https://youtu.be/5sRqythe6TE">demo</a> | -->
    <!-- <a href="https://github.com/codyhouff/Life_in_weeks">code</a> -->

    <p align="justify" style="display: none;" id="life_in_weeks_abs">Applied machine learning to robotics. <br>Led projects, designed and trained models, implemented interpretability tools, collected and curated video datasets, designed data capture hardware and protocol. <br>Led a lab reading group focusing on transformers, RL, and current robotics papers </p>

<pre xml:space="preserve" style="display:none;">  N/A
</pre>
    </div>
  </td>
</tr>

<!-- #4 -->
<tbody><tr>
  <td width="40%" valign="top" align="center"><a href="https://www.linkedin.com/in/cody-houff-474a9017b/">
  <!-- <video autoplay="" loop="" muted="" src="./documents/video/vision-loco.mp4" alt="sym" style="padding-top:0px;padding-bottom:0px;border-radius:15px;" width="90%"></video> -->
  <img src="./documents/images/white.png" alt="gra" style="padding-top:0px;padding-bottom:0px;border-radius:15px;" width="90%">

  </a></td>
  <td width="67%" valign="top">
    <p><a href="https://www.linkedin.com/in/cody-houff-474a9017b/" id="GRA">
    <heading>Advanced Math Tutor</heading></a><br>
    2015 - 2021&nbsp;<b></b><br>
    <b></b>
    </p>

    <div class="paper" id="gra">
    <!-- <a href="https://codyhouff.github.io/Life_in_weeks/">webpage</a> |-->
    <a href="javascript:toggleblock(&#39;life_in_weeks_abs&#39;)">summary</a>
    <!-- <a shape="rect" href="javascript:togglebib(&#39;life_in_weeks&#39;)" class="togglebib">bibtex</a> | -->
    <!-- <a href="https://arxiv.org/abs/2211.07638">arXiv</a> | -->
    <!-- <a href="https://youtu.be/5sRqythe6TE">demo</a> | -->
    <!-- <a href="https://github.com/codyhouff/Life_in_weeks">code</a> -->

    <p align="justify" style="display: none;" id="life_in_weeks_abs">Applied machine learning to robotics. <br>Led projects, designed and trained models, implemented interpretability tools, collected and curated video datasets, designed data capture hardware and protocol. <br>Led a lab reading group focusing on transformers, RL, and current robotics papers </p>

<pre xml:space="preserve" style="display:none;">  N/A
</pre>
    </div>
  </td>
</tr>

<!-- #5 -->
<tbody><tr>
  <td width="40%" valign="top" align="center"><a href="https://www.linkedin.com/in/cody-houff-474a9017b/">
  <!-- <video autoplay="" loop="" muted="" src="./documents/video/vision-loco.mp4" alt="sym" style="padding-top:0px;padding-bottom:0px;border-radius:15px;" width="90%"></video> -->
  <img src="./documents/images/white.png" alt="gra" style="padding-top:0px;padding-bottom:0px;border-radius:15px;" width="90%">

  </a></td>
  <td width="67%" valign="top">
    <p><a href="https://www.linkedin.com/in/cody-houff-474a9017b/" id="GRA">
    <heading>Mechanical Engineer - Protomet Manufacturing</heading></a><br>
    Summer 2018&nbsp;<b></b><br>
    <b></b>
    </p>

    <div class="paper" id="gra">
    <!-- <a href="https://codyhouff.github.io/Life_in_weeks/">webpage</a> |-->
    <a href="javascript:toggleblock(&#39;life_in_weeks_abs&#39;)">summary</a>
    <!-- <a shape="rect" href="javascript:togglebib(&#39;life_in_weeks&#39;)" class="togglebib">bibtex</a> | -->
    <!-- <a href="https://arxiv.org/abs/2211.07638">arXiv</a> | -->
    <!-- <a href="https://youtu.be/5sRqythe6TE">demo</a> | -->
    <!-- <a href="https://github.com/codyhouff/Life_in_weeks">code</a> -->

    <p align="justify" style="display: none;" id="life_in_weeks_abs">Applied machine learning to robotics. <br>Led projects, designed and trained models, implemented interpretability tools, collected and curated video datasets, designed data capture hardware and protocol. <br>Led a lab reading group focusing on transformers, RL, and current robotics papers </p>

<pre xml:space="preserve" style="display:none;">  N/A
</pre>
    </div>
  </td>
</tr>

<!-- #6 -->
<tbody><tr>
  <td width="40%" valign="top" align="center"><a href="https://www.linkedin.com/in/cody-houff-474a9017b/">
  <!-- <video autoplay="" loop="" muted="" src="./documents/video/vision-loco.mp4" alt="sym" style="padding-top:0px;padding-bottom:0px;border-radius:15px;" width="90%"></video> -->
  <img src="./documents/images/white.png" alt="gra" style="padding-top:0px;padding-bottom:0px;border-radius:15px;" width="90%">

  </a></td>
  <td width="67%" valign="top">
    <p><a href="https://www.linkedin.com/in/cody-houff-474a9017b/" id="GRA">
    <heading>Engineer - Oak Ridge National Laboratory</heading></a><br>
    Summer 2016&nbsp;<b></b><br>
    <b></b>
    </p>

    <div class="paper" id="gra">
    <!-- <a href="https://codyhouff.github.io/Life_in_weeks/">webpage</a> |-->
    <a href="javascript:toggleblock(&#39;life_in_weeks_abs&#39;)">summary</a>
    <!-- <a shape="rect" href="javascript:togglebib(&#39;life_in_weeks&#39;)" class="togglebib">bibtex</a> | -->
    <!-- <a href="https://arxiv.org/abs/2211.07638">arXiv</a> | -->
    <!-- <a href="https://youtu.be/5sRqythe6TE">demo</a> | -->
    <!-- <a href="https://github.com/codyhouff/Life_in_weeks">code</a> -->

    <p align="justify" style="display: none;" id="life_in_weeks_abs">Applied machine learning to robotics. <br>Led projects, designed and trained models, implemented interpretability tools, collected and curated video datasets, designed data capture hardware and protocol. <br>Led a lab reading group focusing on transformers, RL, and current robotics papers </p>

<pre xml:space="preserve" style="display:none;">  N/A
</pre>
    </div>
  </td>
</tr>

</tbody></table>
<hr>

<!-- Education -->
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tbody><tr><td><sectionheading>&nbsp;&nbsp;Education</sectionheading></td></tr> <!-- (representative papers are <span class="highlight">highlighted -->
</tbody></table>
<br><table width="100%" align="center" border="0" cellspacing="0" cellpadding="15">

<!-- #1 -->
<tbody><tr>
  <td width="40%" valign="top" align="center"><a href="https://www.gatech.edu/">
  <img src="./documents/images/georgia_tech.png" alt="georgia_tech" style="padding-top:0px;padding-bottom:0px;border-radius:15px;" width="90%">
  </a></td>
  <td width="67%" valign="top">
    <p><a href="https://www.gatech.edu/" id="GEORGIA_TECH">
    <heading>Georgia Institute of Technology</heading></a><br>
    M.S. Robotics concentration in AI, Computer Vision, Mechanics, & Controls<br>
    GPA: 3.83<br>
    2021 - 2023 &nbsp;<b></b><br>
    <b></b>
    </p>

    <div class="paper" id="georgia_tech">
    <!-- <a href="https://github.com/jeremy-collins/vroom">webpage</a> | -->
    <!-- <a href="javascript:toggleblock(&#39;georgia_tech_abs&#39;)">abstract</a> | -->
    <!-- <a shape="rect" href="javascript:togglebib(&#39;georgia_tech&#39;)" class="togglebib">bibtex</a> | -->
    <!-- <a href="./documents/pdfs/video_demonstrations.pdf">pdf</a> | -->
    <!-- <a href="https://www.youtube.com/watch?v=KbrXWbp9Awc">demo</a> | -->
    <!-- <a href="https://github.com/jeremy-collins/vroom">code</a> -->

    <p align="justify" style="display: none;" id="georgia_tech_abs">Training an agent to solve a wide variety of complextasks has remained an open problem for decades; long-horizon, reward-sparse environments are notoriously diffi-cult to learn from scratch via reinforcement learning. Onesuch domain is robotic manipulation, which has been along-standing problem facing classical control theoristsand roboticists alike. At the core of this problem is the curseof dimensionality – the space of possible trajectories is solarge that a complete search through this space to learn apolicy is intractable for even short-horizon tasks. Addition-ally, very precise sensing is often required for robustness butoften leads to expensive solutions that are prone to hard-ware failure. We present an alternative approach that usesvisual data alone to learn a control policy for a robotic armby observing expert video demonstrations. We implementand test several models, accomplishing an 85% success fora pick-and-place task.</p>

<pre xml:space="preserve" style="display:none;">  N/A
</pre>
    </div>
  </td>
</tr>


<!-- #2 -->
<tbody><tr>
  <td width="40%" valign="top" align="center"><a href="https://www.tntech.edu/">
  <!-- <video autoplay="" loop="" muted="" src="./documents/video/vision-loco.mp4" alt="sym" style="padding-top:0px;padding-bottom:0px;border-radius:15px;" width="90%"></video> -->
  <img src="./documents/images/tennessee_tech.png" alt="georgia_tech" style="padding-top:0px;padding-bottom:0px;border-radius:15px;" width="90%">

  </a></td>
  <td width="67%" valign="top">
    <p><a href="https://www.tntech.edu/" id="TENNESSEE_TECH">
    <heading>Tennessee Technological University</heading></a><br>
    B.S. Mechanical Engineering concentration in Mechatronics<br>
    GPA: 3.95<br>
    2015 - 2019&nbsp;<b></b><br>
    <b></b>
    </p>

    <div class="paper" id="tennessee_tech">
    <!-- <a href="https://codyhouff.github.io/Life_in_weeks/">webpage</a> | -->
    <!-- <a href="javascript:toggleblock(&#39;tennessee_tech_abs&#39;)">summary</a> | -->
    <!-- <a shape="rect" href="javascript:togglebib(&#39;tennessee_tech&#39;)" class="togglebib">bibtex</a> | -->
    <!-- <a href="https://arxiv.org/abs/2211.07638">arXiv</a> | -->
    <!-- <a href="https://youtu.be/5sRqythe6TE">demo</a> | -->
    <!-- <a href="https://github.com/codyhouff/Life_in_weeks">code</a> -->

    <p align="justify" style="display: none;" id="tennessee_tech_abs"></p>

<pre xml:space="preserve" style="display:none;">  N/A
</pre>
    </div>
  </td>
</tr>


</tbody></table>
<hr>

<!-- <hr/>

<table width="100%" align="center" border="0" cellpadding="10">
  <tr><td>
    <sectionheading>&nbsp;&nbsp;Selected Awards</sectionheading>
    <ul>
    <li> Google Faculty Research Award (2020)</li>
    <li> Facebook Graduate Fellowship (2018-2020)</li>
    <li> Nvidia Graduate Fellowship (2017-2018)</li>
    <li> Snapchat Inc. Graduate Fellowship (2017)</li>
    <li> Gold Medal in Computer Science at IIT Kanpur (2014)</li>
    <li> Best Undergraduate Thesis Award at IIT Kanpur (2014)</li>
    </ul>
  </td></tr>
</table>
-->
<!-- <hr> -->

<!--
<script xml:space="preserve" language="JavaScript">
hideallbibs();
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('jpm15_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('fg15_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('wacv15_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('iclr15_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('cvpr15_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('iccv15_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('jmlr16_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('cvpr16_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('iclr16_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('cvpr17_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('icml17_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('nips17_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('iclr18_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('icml18_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('cvprw18_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('iclr19_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('assemblies19_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('sgm20_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('compgan18_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('icml19_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('phd19_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('neurips19_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('plan2explore_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('icml20_smp_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('uai20_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('neurips20_ndp_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('iclr21_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('worldsheet_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('flavr_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('icra21_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('ral21_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('keypoint3D_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('spt_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('hndp_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('rma_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('energyloco_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('lexa_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('raps_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('lff_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('envexplore_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('rtk_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('whirl_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('tars_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('clear_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('rb2_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('navloco_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('gptplanner_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('geometrydex_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('revolver_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('eccv22_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('clipICML22_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('arma_abs');
</script>
-->


<!-- <iframe scrolling="no" frameborder="0" allowtransparency="true" src="./Deepak Pathak_files/widget_iframe.2b2d73daf636805223fb11d48f3e94f7.html" title="Twitter settings iframe" style="display: none;"></iframe><iframe id="rufous-sandbox" scrolling="no" frameborder="0" allowtransparency="true" allowfullscreen="true" style="position: absolute; visibility: hidden; display: none; width: 0px; height: 0px; padding: 0px; border: none;" title="Twitter analytics iframe" src="./Deepak Pathak_files/saved_resource.html"></iframe></body></html> -->
